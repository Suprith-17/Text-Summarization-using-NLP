# Text-Summarization-using-NLP
Text Summarization using Natural Language Processing


# Text Summarization Using NLP ğŸ¤–ğŸ“š

## Overview
This thesis explores text summarization techniques using Natural Language Processing (NLP). The focus is on leveraging advanced neural network architectures, including LSTM, BiLSTM, and attention mechanisms with GloVe embeddings, to generate concise summaries of text.

## Techniques Used
1. **LSTM (Long Short-Term Memory)** ğŸ§ 
   - Utilizes memory cells to capture long-range dependencies in sequences.

2. **BiLSTM (Bidirectional LSTM)** ğŸ”„
   - Processes input data in both forward and backward directions, improving context understanding.

3. **LSTM with GloVe Attention** ğŸŒŸ
   - Integrates GloVe embeddings with attention mechanisms to enhance summary quality.

4. **BiLSTM with GloVe Attention** âœ¨
   - Combines BiLSTM architecture with attention and GloVe embeddings for better performance.

## Evaluation Metrics
To evaluate the performance of the summarization models, we employed the **ROUGE Score** (Recall-Oriented Understudy for Gisting Evaluation) ğŸ“, which measures the quality of summaries by comparing them to reference summaries.

## Installation
To run the code, please make sure you have the following libraries installed:

```bash
pip install numpy pandas nltk tensorflow keras scikit-learn

